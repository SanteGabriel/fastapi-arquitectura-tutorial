# ðŸ¤– Chat Service - Servicio de IntegraciÃ³n LLMs

## ðŸ“‹ DescripciÃ³n
Microservicio responsable de la integraciÃ³n con mÃºltiples proveedores de LLMs (DeepSeek, Claude, ChatGPT, Gemini) para el sistema LLM Wrapper Web.

## ðŸ—ï¸ Arquitectura del Servicio

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CHAT SERVICE               â”‚
â”‚              Puerto: 8002               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Routes    â”‚    â”‚  Middleware â”‚    â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚
â”‚  â”‚ /chat       â”‚â—„â”€â”€â–ºâ”‚ Auth Check  â”‚    â”‚
â”‚  â”‚ /models     â”‚    â”‚ Rate Limit  â”‚    â”‚
â”‚  â”‚ /stream     â”‚    â”‚ Input Valid â”‚    â”‚
â”‚  â”‚ /history    â”‚    â”‚ Token Count â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                   â”‚          â”‚
â”‚         â–¼                   â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚LLM Providersâ”‚    â”‚ LLM Router  â”‚    â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚
â”‚  â”‚ OpenAI      â”‚â—„â”€â”€â–ºâ”‚ Load Balancerâ”‚   â”‚
â”‚  â”‚ Claude      â”‚    â”‚ Fallback    â”‚    â”‚
â”‚  â”‚ DeepSeek    â”‚    â”‚ Selection   â”‚    â”‚
â”‚  â”‚ Gemini      â”‚    â”‚             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                   â”‚          â”‚
â”‚         â–¼                   â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Response   â”‚    â”‚   Utils     â”‚    â”‚
â”‚  â”‚  Processor  â”‚    â”‚             â”‚    â”‚
â”‚  â”‚             â”‚    â”‚ Token Count â”‚    â”‚
â”‚  â”‚ Streaming   â”‚    â”‚ Sanitizer   â”‚    â”‚
â”‚  â”‚ Formatting  â”‚    â”‚ Validator   â”‚    â”‚
â”‚  â”‚ Error Handleâ”‚    â”‚ Metrics     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Funcionalidades

### Core Features
- **Multi-LLM Integration**: Soporte para 4 proveedores principales
- **Smart Routing**: SelecciÃ³n automÃ¡tica o manual de LLM
- **Streaming Responses**: Respuestas en tiempo real
- **Fallback System**: Cambio automÃ¡tico si un LLM falla
- **Rate Limiting**: Control por suscripciÃ³n de usuario
- **Token Counting**: Conteo preciso de tokens

### Endpoints Principales

| MÃ©todo | Endpoint | DescripciÃ³n | Auth Requerida |
|--------|----------|-------------|----------------|
| POST | `/chat` | Enviar mensaje al LLM | âœ… |
| POST | `/chat/stream` | Chat con streaming | âœ… |
| GET | `/models` | Listar LLMs disponibles | âœ… |
| GET | `/models/{model}/status` | Estado de un LLM | âœ… |
| POST | `/chat/batch` | Procesamiento por lotes | âœ… |
| GET | `/usage` | EstadÃ­sticas de uso | âœ… |

## ðŸ¤– LLMs Integrados

### OpenAI (ChatGPT)
```python
class OpenAIProvider:
    models = ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"]
    max_tokens = 128000
    supports_streaming = True
    cost_per_1k_tokens = {"input": 0.01, "output": 0.03}
```

### Anthropic (Claude)
```python
class ClaudeProvider:
    models = ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku"]
    max_tokens = 200000
    supports_streaming = True
    cost_per_1k_tokens = {"input": 0.015, "output": 0.075}
```

### DeepSeek
```python
class DeepSeekProvider:
    models = ["deepseek-chat", "deepseek-coder"]
    max_tokens = 32000
    supports_streaming = True
    cost_per_1k_tokens = {"input": 0.0014, "output": 0.0028}
```

### Google (Gemini)
```python
class GeminiProvider:
    models = ["gemini-pro", "gemini-pro-vision", "gemini-ultra"]
    max_tokens = 32768
    supports_streaming = True
    cost_per_1k_tokens = {"input": 0.0005, "output": 0.0015}
```

## ðŸ“Š Modelos de Datos

### Chat Request
```python
class ChatRequest(BaseModel):
    message: str
    model: Optional[str] = None  # Auto-select si es None
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    stream: bool = False
    conversation_id: Optional[str] = None
    system_prompt: Optional[str] = None
```

### Chat Response
```python
class ChatResponse(BaseModel):
    message: str
    model_used: str
    tokens_used: int
    cost_estimate: float
    conversation_id: str
    timestamp: datetime
    processing_time: float
```

### LLM Status
```python
class LLMStatus(BaseModel):
    provider: str
    model: str
    status: str  # online, offline, maintenance
    response_time_avg: float
    error_rate: float
    last_check: datetime
```

## ðŸ”§ ConfiguraciÃ³n

### Variables de Entorno (.env)
```env
# LLM API Keys
OPENAI_API_KEY=sk-your-openai-api-key
CLAUDE_API_KEY=sk-ant-your-claude-api-key
DEEPSEEK_API_KEY=your-deepseek-api-key
GEMINI_API_KEY=your-gemini-api-key

# LLM Configuration
DEFAULT_LLM=openai
MAX_TOKENS_DEFAULT=4000
TEMPERATURE_DEFAULT=0.7
ENABLE_STREAMING=true

# Rate Limiting (por plan de suscripciÃ³n)
FREE_REQUESTS_PER_HOUR=50
PREMIUM_REQUESTS_PER_HOUR=500
ENTERPRISE_REQUESTS_PER_HOUR=5000

# Failover Configuration
ENABLE_FALLBACK=true
FALLBACK_ORDER=openai,claude,deepseek,gemini
MAX_RETRY_ATTEMPTS=3
RETRY_DELAY_SECONDS=2

# Monitoring
ENABLE_METRICS=true
LOG_REQUESTS=true
LOG_RESPONSES=false  # Para privacidad
```

### Dependencias (requirements.txt)
```
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
openai==1.6.1
anthropic==0.8.1
httpx==0.26.0
aiohttp==3.9.1
tiktoken==0.5.2
google-generativeai==0.3.2
tenacity==8.2.3
slowapi==0.1.9
prometheus-client==0.19.0
```

## ðŸ”€ LLM Router Logic

### Auto-Selection Algorithm
```python
async def select_optimal_llm(
    message: str, 
    user_subscription: str,
    conversation_context: Optional[List]
) -> str:
    """
    LÃ³gica de selecciÃ³n automÃ¡tica de LLM basada en:
    - Tipo de consulta (cÃ³digo, conversaciÃ³n, anÃ¡lisis)
    - Plan de suscripciÃ³n del usuario
    - Estado actual de los LLMs
    - Costos por token
    - Historial de respuestas exitosas
    """
    
    # AnÃ¡lisis del tipo de consulta
    if is_code_related(message):
        return "deepseek"  # Mejor para cÃ³digo
    elif is_long_conversation(conversation_context):
        return "claude"  # Mejor contexto largo
    elif user_subscription == "free":
        return "deepseek"  # MÃ¡s econÃ³mico
    else:
        return "openai"  # Default confiable
```

### Fallback Strategy
```python
async def handle_llm_fallback(
    request: ChatRequest,
    failed_provider: str,
    error: Exception
) -> ChatResponse:
    """
    Sistema de fallback cuando un LLM falla:
    1. Log del error
    2. Seleccionar siguiente LLM en orden
    3. Ajustar parÃ¡metros si es necesario
    4. Reintentar con nuevo proveedor
    """
    fallback_order = get_fallback_order(failed_provider)
    
    for next_provider in fallback_order:
        if await is_provider_healthy(next_provider):
            return await send_to_llm(request, next_provider)
    
    raise AllProvidersUnavailableException()
```

## ðŸ“ˆ Rate Limiting & Quotas

### Por Plan de SuscripciÃ³n
```python
RATE_LIMITS = {
    "free": {
        "requests_per_hour": 50,
        "tokens_per_day": 10000,
        "available_models": ["deepseek", "gemini"]
    },
    "premium": {
        "requests_per_hour": 500,
        "tokens_per_day": 100000,
        "available_models": ["openai", "claude", "deepseek", "gemini"]
    },
    "enterprise": {
        "requests_per_hour": 5000,
        "tokens_per_day": 1000000,
        "available_models": ["*"],
        "priority_queue": True
    }
}
```

## ðŸ”’ Seguridad y ValidaciÃ³n

### Input Sanitization
```python
async def sanitize_user_input(message: str) -> str:
    """
    SanitizaciÃ³n de inputs para prevenir:
    - Injection prompts
    - Contenido malicioso
    - Prompts que violen ToS de LLMs
    """
    # Remover caracteres peligrosos
    # Detectar prompt injection
    # Validar longitud
    # Filtrar contenido inapropiado
    return cleaned_message
```

### Content Filtering
```python
BLOCKED_PATTERNS = [
    "ignore previous instructions",
    "act as if you are",
    "pretend to be",
    # ... mÃ¡s patrones de prompt injection
]

async def validate_content(message: str) -> bool:
    """Validar que el contenido cumple polÃ­ticas"""
    return not any(pattern in message.lower() for pattern in BLOCKED_PATTERNS)
```

## ðŸ§ª Testing

### Unit Tests
```bash
# Test individual LLM providers
pytest tests/test_providers/ -v

# Test routing logic
pytest tests/test_router.py -v

# Test rate limiting
pytest tests/test_rate_limiting.py -v

# Test streaming
pytest tests/test_streaming.py -v
```

### Integration Tests
```bash
# Test con LLMs reales (requiere API keys)
pytest tests/integration/ -v --api-keys

# Mock tests (sin API keys)
pytest tests/integration/ -v --mock
```

## ðŸš€ Desarrollo Local

### Setup Inicial
```bash
# Crear entorno virtual
python -m venv .venv
source .venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
cp .env.example .env
# Editar .env con tus API keys

# Ejecutar servicio
uvicorn main:app --reload --port 8002
```

### Health Check
```bash
curl http://localhost:8002/health
curl http://localhost:8002/models
```

## ðŸ“Š Monitoreo y MÃ©tricas

### MÃ©tricas Clave
- Requests por segundo por LLM
- Tiempo de respuesta promedio
- Tasa de errores por proveedor
- Tokens consumidos por usuario
- Costos por LLM
- Uso de fallbacks

### Prometheus Metrics
```python
from prometheus_client import Counter, Histogram, Gauge

llm_requests_total = Counter("llm_requests_total", "Total requests", ["provider", "model"])
llm_response_time = Histogram("llm_response_time_seconds", "Response time", ["provider"])
llm_tokens_used = Counter("llm_tokens_used_total", "Tokens used", ["provider", "user"])
llm_errors_total = Counter("llm_errors_total", "Total errors", ["provider", "error_type"])
```

---

**Puerto**: 8002  
**Estado**: ðŸ”„ En desarrollo  
**PrÃ³ximo**: Implementar integraciÃ³n con OpenAI 